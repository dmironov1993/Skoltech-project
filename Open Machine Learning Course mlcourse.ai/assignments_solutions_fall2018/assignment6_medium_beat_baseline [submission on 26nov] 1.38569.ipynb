{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). Edited by Sergey Kolchenko (@KolchenkoSergey). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6\n",
    "### <center> Beating baselines in \"How good is your Medium article?\"\n",
    "    \n",
    "<img src='../../img/medium_claps.jpg' width=40% />\n",
    "\n",
    "\n",
    "[Competition](https://www.kaggle.com/c/how-good-is-your-medium-article). The task is to beat \"A6 baseline\" (~1.45 Public LB score). Do not forget about our shared [\"primitive\" baseline](https://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline) - you'll find something valuable there.\n",
    "\n",
    "**Your task:**\n",
    " 1. \"Freeride\". Come up with good features to beat the baseline \"A6 baseline\" (for now, public LB is only considered)\n",
    " 2. You need to name your [team](https://www.kaggle.com/c/how-good-is-your-medium-article/team) (out of 1 person) in full accordance with the [course rating](https://drive.google.com/open?id=19AGEhUQUol6_kNLKSzBsjcGUU3qWy3BNUg8x8IFkO3Q). You can think of it as a part of the assignment. 16 credits for beating the mentioned baseline and correct team naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge cld2-cffi --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import cld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../../../Kaggle/Medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), index_col='id')\n",
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62313,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "del train['Unnamed: 0']\n",
    "del test['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 8), (34645, 8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat((train, pd.DataFrame(y_train)), axis=1)\n",
    "train['published'] = train['published'].apply(pd.to_datetime)\n",
    "train['year'] = train['published'].apply(lambda x: x.year)\n",
    "train = train[train['year']>=2016]\n",
    "train.sort_values(by='published', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[0]\n",
    "train.drop(columns={0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45938,), (45938, 9))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge langdetect --yes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['published'] = train['published'].apply(pd.to_datetime)  used above\n",
    "train['tags'] = train['tags'].astype('str')\n",
    "train['dow'] = train['published'].apply(lambda x: x.dayofweek)\n",
    "train['hour'] = train['published'].apply(lambda x: x.hour)\n",
    "train['month'] = train['published'].apply(lambda x: x.month)\n",
    "#train['year'] = train['published'].apply(lambda x: x.year)    #used above\n",
    "train['number_of_tags'] = train['tags'].apply(lambda x: len(x.split()))\n",
    "\n",
    "test['tags'] = test['tags'].astype('str')\n",
    "test['published'] = test['published'].apply(pd.to_datetime)\n",
    "test['dow'] = test['published'].apply(lambda x: x.dayofweek)\n",
    "test['hour'] = test['published'].apply(lambda x: x.hour)\n",
    "test['month'] = test['published'].apply(lambda x: x.month)\n",
    "test['year'] = test['published'].apply(lambda x: x.year)\n",
    "test['number_of_tags'] = test['tags'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the following groups of features:**\n",
    "    - Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "    - Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title_length'] = train['title'].apply(lambda x: len(x.split(' ')))\n",
    "train['title_length_sq'] = train['title_length'].apply(lambda x: x**2)\n",
    "\n",
    "test['title_length'] = test['title'].apply(lambda x: len(x.split(' ')))\n",
    "test['title_length_sq'] = test['title_length'].apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['topics'] = train['url'].apply(lambda x: x.split('/')[3] if len(x.split('/')) > 3 else 'None')\n",
    "test['topics'] = test['url'].apply(lambda x: x.split('/')[3] if len(x.split('/')) > 3 else 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['length_log'] = train['length'].apply(lambda x: np.log(x))\n",
    "test['length_log'] = test['length'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.sort_values(by='published')\n",
    "#test = test.sort_values(by='published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df_feats = full_df.copy() \n",
    "#full_df_feats.drop(columns=['content','published','title','domain','tags','length','url','hour','year'], inplace=True) \n",
    "#list_to_dums = ['author','dow', 'month', 'number_of_tags'] \n",
    "#full_df_feats = pd.get_dummies(full_df_feats, columns = list_to_dums, drop_first=True, prefix=list_to_dums, sparse=False); \n",
    "#cv_content = TfidfVectorizer(ngram_range=(1, 2), max_features=43908) \n",
    "#cv_title = TfidfVectorizer(ngram_range=(1, 2), max_features=43908) ; \n",
    "#'alpha': 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeansshow(k,X):\n",
    "\n",
    "    from sklearn import cluster\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    kmeans = cluster.KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    #print centroids\n",
    "\n",
    "    for i in range(k):\n",
    "        # select only data observations with cluster label == i\n",
    "        ds = X[np.where(labels==i)]\n",
    "        # plot the data observations\n",
    "        pyplot.plot(ds[:,0],ds[:,1],'o')\n",
    "        # plot the centroids\n",
    "        lines = pyplot.plot(centroids[i,0],centroids[i,1],'kx')\n",
    "        # make the centroid x's bigger\n",
    "        pyplot.setp(lines,ms=15.0)\n",
    "        pyplot.setp(lines,mew=2.0)\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "train['hour_sin_x'] = train['hour'].apply(lambda ts: np.sin(2*pi*ts/24.))\n",
    "train['hour_cos_x'] = train['hour'].apply(lambda ts: np.cos(2*pi*ts/24.))\n",
    "\n",
    "test['hour_sin_x'] = test['hour'].apply(lambda ts: np.sin(2*pi*ts/24.))\n",
    "test['hour_cos_x'] = test['hour'].apply(lambda ts: np.cos(2*pi*ts/24.))\n",
    "\n",
    "train['month_sin_x'] = train['month'].apply(lambda ts: np.sin(2*pi*ts/24.))\n",
    "train['month_cos_x'] = train['month'].apply(lambda ts: np.cos(2*pi*ts/24.))\n",
    "\n",
    "test['month_sin_x'] = test['month'].apply(lambda ts: np.sin(2*pi*ts/24.))\n",
    "test['month_cos_x'] = test['month'].apply(lambda ts: np.cos(2*pi*ts/24.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split = len(train)\n",
    "full_df = pd.concat((train, test), sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df_feats = full_df.copy()\n",
    "#full_df_feats.drop(columns=['content', 'published', 'title', 'length',\n",
    "#       'url', 'dow', 'hour', 'month', 'year', 'number_of_tags', 'title_length'],  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_to_dums = ['author','tags','domain','topics','length_log','title_length_sq']\n",
    "#full_df_feats = pd.get_dummies(full_df_feats, columns = list_to_dums, drop_first=True, prefix=list_to_dums, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'content', 'domain', 'dow', 'hour', 'hour_cos_x',\n",
       "       'hour_sin_x', 'length', 'length_log', 'month', 'month_cos_x',\n",
       "       'month_sin_x', 'number_of_tags', 'published', 'tags', 'title',\n",
       "       'title_length', 'title_length_sq', 'topics', 'url', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_feats = full_df.copy() \n",
    "full_df_feats.drop(columns=['content', 'published','length', \n",
    "                            'url','hour','year','title_length', 'title_length_sq', 'dow', 'tags', \n",
    "                            'month', 'number_of_tags', 'length_log','title'], inplace=True) \n",
    "list_to_dums = ['author', 'topics', 'domain'] \n",
    "full_df_feats = pd.get_dummies(full_df_feats, columns = list_to_dums, drop_first=True, prefix=list_to_dums, sparse=False); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80583, 75328)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80583, 75324)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full_df_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_content = TfidfVectorizer(ngram_range=(1, 2), max_features=300000, sublinear_tf=True) \n",
    "cv_title = TfidfVectorizer(ngram_range=(1, 2), max_features=300000, sublinear_tf=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_content = cv_content.fit_transform(full_df.iloc[:idx_split,:]['content'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_content = cv_content.transform(full_df.iloc[idx_split:,:]['content'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title = cv_title.fit_transform(full_df.iloc[:idx_split,:]['title'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_title = cv_title.transform(full_df.iloc[idx_split:,:]['title'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = hstack([X_train_content, \n",
    "                         X_train_title,\n",
    "                         full_df_feats.iloc[:idx_split,:]]).tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sparse = hstack([X_test_content, \n",
    "                        X_test_title,\n",
    "                        full_df_feats.iloc[idx_split:,:]]).tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((7658,), (7656,)),\n",
       " ((15314,), (7656,)),\n",
       " ((22970,), (7656,)),\n",
       " ((30626,), (7656,)),\n",
       " ((38282,), (7656,))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(el[0].shape, el[1].shape) for el in time_split.split(X_train_sparse)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_grid = GridSearchCV(estimator=Ridge(random_state=17), \n",
    "                          param_grid = {'alpha': [0.06, 0.08, 0.1, 0.12, 0.14]}, \n",
    "                          scoring='neg_mean_absolute_error',\n",
    "                          n_jobs=1, \n",
    "                          cv=time_split, \n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 37.5min finished\n"
     ]
    }
   ],
   "source": [
    "#ridge = Ridge(random_state=17, alpha=2)                          \n",
    "ridge_pred_test = ridge_grid.fit(X_train_sparse, y_train).predict(X_test_sparse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.12, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=17, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0589594630908086"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_predictions = ridge_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = ridge_pred_test + (4.33328 - np.mean(ridge_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.333279999999999"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample=os.path.join(PATH_TO_DATA, \n",
    "                                                      'sample_submission.csv')):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_submission_file(y_predictions, os.path.join(PATH_TO_DATA,\n",
    "#                                                    'to_test26nov-1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(y_predictions, os.path.join(PATH_TO_DATA,\n",
    "                                                    'to_a_test_26-final.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the assignment. Much more credits will be given to the winners in this competition, check [course roadmap](https://mlcourse.ai/roadmap). Do not spoil the assignment and the competition - don't share high-performing kernels (with MAE < 1.5).\n",
    "\n",
    "Some ideas for improvement:\n",
    "\n",
    "- Engineer good features, this is the key to success. Some simple features will be based on publication time, authors, content length and so on\n",
    "- You may not ignore HTML and extract some features from there\n",
    "- You'd better experiment with your validation scheme. You should see a correlation between your local improvements and LB score\n",
    "- Try TF-IDF, ngrams, Word2Vec and GloVe embeddings\n",
    "- Try various NLP techniques like stemming and lemmatization\n",
    "- Tune hyperparameters. In our example, we've left only 50k features and used C=1 as a regularization parameter, this can be changed\n",
    "- SGD and Vowpal Wabbit will learn much faster\n",
    "- Play around with blending and/or stacking. An intro is given in [this Kernel](https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-simple-blending) by @yorko \n",
    "- In our course, we don't cover neural nets. But it's not obliged to use GRUs/LSTMs/whatever in this competition.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "<img src='../../img/kaggle_shakeup.png' width=50%>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
